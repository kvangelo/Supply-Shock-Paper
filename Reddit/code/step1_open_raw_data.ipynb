{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through a folder of Reddit submissions and comments. \n",
    "First, torrent the reddit data, following bleow instructions. Unzip the files, and save all relevant files under the name \"raw_reddit_data\" or within a folder named \"raw_reddit_data\".\n",
    "Save the following .ipynb file in the same folder and execute it using python\n",
    "\n",
    "Code is adapted from: https://academictorrents.com/details/1614740ac8c94505e4ecb9d88be8bed7b6afddd4\n",
    "Detailed Instructions: https://www.reddit.com/r/pushshift/comments/1itme1k/separate_dump_files_for_the_top_40k_subreddits/\n",
    "\n",
    "\n",
    "Prepared by Kasey Vangelov (kvangelo@umd.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 11:25:27,147 - INFO: Filtering field: None\n",
      "2025-04-16 11:25:27,184 - INFO: On values: \n",
      "2025-04-16 11:25:27,187 - INFO: Exact match off. Single field None.\n",
      "2025-04-16 11:25:27,187 - INFO: From date 2020-12-31 to date 2025-01-01\n",
      "2025-04-16 11:25:27,187 - INFO: Output format set to csv\n",
      "2025-04-16 11:25:27,203 - INFO: Processing 12 files\n",
      "2025-04-16 11:25:27,204 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\opiates_submissions.zst : Output: 1_collect_pushshift\\opiates_submissions.csv : Is submission True\n",
      "C:\\Users\\vange\\AppData\\Local\\Temp\\ipykernel_25340\\2436246181.py:176: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  created = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
      "2025-04-16 11:25:31,465 - INFO: 2017-01-20 20:57:18 : 100,000 : 0 : 0 : 58,328,375:50%\n",
      "2025-04-16 11:25:34,088 - INFO: 2019-02-04 20:47:40 : 200,000 : 0 : 0 : 75,499,200:65%\n",
      "2025-04-16 11:25:37,727 - INFO: 2022-01-23 21:28:55 : 300,000 : 29,693 : 0 : 103,024,950:89%\n",
      "2025-04-16 11:25:42,135 - INFO: Complete : 368,904 : 98,598 : 0\n",
      "2025-04-16 11:25:42,144 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\cocaine_comments.zst : Output: 1_collect_pushshift\\cocaine_comments.csv : Is submission False\n",
      "2025-04-16 11:25:43,346 - INFO: 2017-04-14 06:33:43 : 100,000 : 0 : 0 : 22,544,900:7%\n",
      "2025-04-16 11:25:43,987 - INFO: 2018-05-23 14:12:14 : 200,000 : 0 : 0 : 22,544,900:7%\n",
      "2025-04-16 11:25:45,308 - INFO: 2019-01-09 05:42:43 : 300,000 : 0 : 0 : 34,472,725:11%\n",
      "2025-04-16 11:25:46,917 - INFO: 2019-07-03 20:05:49 : 400,000 : 0 : 0 : 45,220,875:15%\n",
      "2025-04-16 11:25:48,842 - INFO: 2019-12-06 05:43:57 : 500,000 : 0 : 0 : 55,313,650:18%\n",
      "2025-04-16 11:25:50,544 - INFO: 2020-04-11 22:21:52 : 600,000 : 0 : 0 : 65,144,275:21%\n",
      "2025-04-16 11:25:52,416 - INFO: 2020-08-11 16:42:19 : 700,000 : 0 : 0 : 74,843,825:24%\n",
      "2025-04-16 11:25:54,041 - INFO: 2020-12-31 14:11:09 : 800,000 : 610 : 0 : 84,936,600:27%\n",
      "2025-04-16 11:25:57,521 - INFO: 2021-04-16 15:23:55 : 900,000 : 100,610 : 0 : 84,936,600:27%\n",
      "2025-04-16 11:26:01,668 - INFO: 2021-07-23 18:03:35 : 1,000,000 : 200,610 : 0 : 94,767,225:31%\n",
      "2025-04-16 11:26:05,656 - INFO: 2021-10-24 22:21:27 : 1,100,000 : 300,610 : 0 : 104,597,850:34%\n",
      "2025-04-16 11:26:09,459 - INFO: 2022-01-03 21:47:48 : 1,200,000 : 400,610 : 0 : 114,428,475:37%\n",
      "2025-04-16 11:26:13,020 - INFO: 2022-03-06 10:10:52 : 1,300,000 : 500,610 : 0 : 124,521,250:40%\n",
      "2025-04-16 11:26:16,551 - INFO: 2022-05-02 05:38:38 : 1,400,000 : 600,610 : 0 : 134,220,800:43%\n",
      "2025-04-16 11:26:19,938 - INFO: 2022-07-03 15:13:57 : 1,500,000 : 700,610 : 0 : 143,920,350:46%\n",
      "2025-04-16 11:26:23,949 - INFO: 2022-08-30 20:15:01 : 1,600,000 : 800,610 : 0 : 153,882,050:50%\n",
      "2025-04-16 11:26:28,515 - INFO: 2022-10-29 00:33:37 : 1,700,000 : 900,610 : 0 : 163,843,750:53%\n",
      "2025-04-16 11:26:32,578 - INFO: 2022-12-26 07:40:05 : 1,800,000 : 1,000,610 : 0 : 173,674,375:56%\n",
      "2025-04-16 11:26:35,880 - INFO: 2023-02-16 04:53:59 : 1,900,000 : 1,100,610 : 0 : 183,242,850:59%\n",
      "2025-04-16 11:26:39,171 - INFO: 2023-04-07 22:03:36 : 2,000,000 : 1,200,610 : 0 : 191,238,425:62%\n",
      "2025-04-16 11:26:42,097 - INFO: 2023-05-24 05:41:01 : 2,100,000 : 1,300,610 : 0 : 198,971,850:64%\n",
      "2025-04-16 11:26:45,589 - INFO: 2023-07-08 19:42:02 : 2,200,000 : 1,400,610 : 0 : 214,831,925:69%\n",
      "2025-04-16 11:26:48,877 - INFO: 2023-08-25 04:03:09 : 2,300,000 : 1,500,610 : 0 : 222,434,275:72%\n",
      "2025-04-16 11:26:51,577 - INFO: 2023-10-13 16:29:02 : 2,400,000 : 1,600,610 : 0 : 230,036,625:74%\n",
      "2025-04-16 11:26:54,896 - INFO: 2023-12-02 08:01:53 : 2,500,000 : 1,700,610 : 0 : 245,765,625:79%\n",
      "2025-04-16 11:26:58,257 - INFO: 2024-01-19 05:21:04 : 2,600,000 : 1,800,610 : 0 : 253,630,125:82%\n",
      "2025-04-16 11:27:01,711 - INFO: 2024-03-12 22:39:17 : 2,700,000 : 1,900,610 : 0 : 261,494,625:84%\n",
      "2025-04-16 11:27:05,186 - INFO: 2024-05-05 18:01:39 : 2,800,000 : 2,000,610 : 0 : 277,092,550:89%\n",
      "2025-04-16 11:27:08,365 - INFO: 2024-06-30 07:18:16 : 2,900,000 : 2,100,610 : 0 : 285,219,200:92%\n",
      "2025-04-16 11:27:11,603 - INFO: 2024-09-02 05:24:50 : 3,000,000 : 2,200,610 : 0 : 293,345,850:95%\n",
      "2025-04-16 11:27:14,753 - INFO: 2024-11-16 02:38:25 : 3,100,000 : 2,300,610 : 0 : 309,751,404:100%\n",
      "2025-04-16 11:27:16,137 - INFO: Complete : 3,153,112 : 2,353,723 : 0\n",
      "2025-04-16 11:27:16,138 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\cocaine_submissions.zst : Output: 1_collect_pushshift\\cocaine_submissions.csv : Is submission True\n",
      "2025-04-16 11:27:18,381 - INFO: 2020-05-09 10:55:49 : 100,000 : 0 : 0 : 28,312,200:21%\n",
      "2025-04-16 11:27:32,619 - INFO: 2021-11-10 03:57:35 : 200,000 : 64,223 : 0 : 48,759,900:36%\n",
      "2025-04-16 11:27:40,113 - INFO: 2022-10-14 04:40:35 : 300,000 : 164,223 : 0 : 69,338,675:51%\n",
      "2025-04-16 11:27:45,594 - INFO: 2023-06-10 02:57:43 : 400,000 : 264,223 : 0 : 77,596,400:57%\n",
      "2025-04-16 11:27:52,192 - INFO: 2023-12-29 06:27:31 : 500,000 : 364,223 : 0 : 106,826,125:78%\n",
      "2025-04-16 11:27:58,246 - INFO: 2024-07-14 19:26:17 : 600,000 : 464,223 : 0 : 125,569,850:92%\n",
      "2025-04-16 11:28:01,889 - INFO: Complete : 656,864 : 521,088 : 0\n",
      "2025-04-16 11:28:01,889 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\fentanyl_comments.zst : Output: 1_collect_pushshift\\fentanyl_comments.csv : Is submission False\n",
      "2025-04-16 11:28:07,228 - INFO: 2022-07-29 17:41:57 : 100,000 : 85,858 : 0 : 25,821,775:60%\n",
      "2025-04-16 11:28:11,677 - INFO: 2023-11-24 21:50:32 : 200,000 : 185,858 : 0 : 37,094,225:87%\n",
      "2025-04-16 11:28:17,247 - INFO: Complete : 284,754 : 270,613 : 0\n",
      "2025-04-16 11:28:17,247 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\fentanyl_submissions.zst : Output: 1_collect_pushshift\\fentanyl_submissions.csv : Is submission True\n",
      "2025-04-16 11:28:19,115 - INFO: Complete : 26,650 : 24,805 : 0\n",
      "2025-04-16 11:28:19,119 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\heroin_comments.zst : Output: 1_collect_pushshift\\heroin_comments.csv : Is submission False\n",
      "2025-04-16 11:28:20,507 - INFO: 2020-05-14 09:24:23 : 100,000 : 0 : 0 : 12,845,350:14%\n",
      "2025-04-16 11:28:22,251 - INFO: 2021-02-02 03:16:15 : 200,000 : 14,489 : 0 : 25,297,475:28%\n",
      "2025-04-16 11:28:32,988 - INFO: 2021-09-09 18:30:01 : 300,000 : 114,489 : 0 : 37,749,600:41%\n",
      "2025-04-16 11:28:41,781 - INFO: 2022-04-13 18:01:23 : 400,000 : 214,489 : 0 : 49,808,500:54%\n",
      "2025-04-16 11:28:51,888 - INFO: 2023-03-04 06:35:21 : 500,000 : 314,489 : 0 : 73,139,850:80%\n",
      "2025-04-16 11:28:59,820 - INFO: 2024-01-08 12:45:54 : 600,000 : 414,489 : 0 : 83,756,925:91%\n",
      "2025-04-16 11:29:04,969 - INFO: Complete : 685,332 : 499,822 : 0\n",
      "2025-04-16 11:29:04,969 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\heroin_submissions.zst : Output: 1_collect_pushshift\\heroin_submissions.csv : Is submission True\n",
      "2025-04-16 11:29:09,618 - INFO: Complete : 63,736 : 42,104 : 0\n",
      "2025-04-16 11:29:09,618 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\MDMA_comments.zst : Output: 1_collect_pushshift\\MDMA_comments.csv : Is submission False\n",
      "2025-04-16 11:29:11,735 - INFO: 2015-02-23 13:58:39 : 100,000 : 0 : 0 : 32,113,375:13%\n",
      "2025-04-16 11:29:12,718 - INFO: 2016-05-20 21:34:28 : 200,000 : 0 : 0 : 32,113,375:13%\n",
      "2025-04-16 11:29:21,297 - INFO: 2017-04-15 10:39:12 : 300,000 : 0 : 0 : 55,706,875:22%\n",
      "2025-04-16 11:29:22,517 - INFO: 2018-01-27 03:20:40 : 400,000 : 0 : 0 : 55,706,875:22%\n",
      "2025-04-16 11:29:23,733 - INFO: 2018-08-22 14:26:23 : 500,000 : 0 : 0 : 69,862,975:28%\n",
      "2025-04-16 11:29:24,933 - INFO: 2019-02-09 22:06:38 : 600,000 : 0 : 0 : 82,315,100:33%\n",
      "2025-04-16 11:29:26,036 - INFO: 2019-06-26 23:46:24 : 700,000 : 0 : 0 : 92,932,175:37%\n",
      "2025-04-16 11:29:27,233 - INFO: 2019-10-31 23:04:18 : 800,000 : 0 : 0 : 103,549,250:41%\n",
      "2025-04-16 11:29:28,450 - INFO: 2020-02-09 04:48:55 : 900,000 : 0 : 0 : 113,904,175:45%\n",
      "2025-04-16 11:29:29,682 - INFO: 2020-05-10 22:01:58 : 1,000,000 : 0 : 0 : 124,259,100:49%\n",
      "2025-04-16 11:29:30,934 - INFO: 2020-08-17 23:44:15 : 1,100,000 : 0 : 0 : 134,482,950:53%\n",
      "2025-04-16 11:29:32,199 - INFO: 2020-12-12 16:07:19 : 1,200,000 : 0 : 0 : 145,100,025:57%\n",
      "2025-04-16 11:29:35,264 - INFO: 2021-05-15 20:29:43 : 1,300,000 : 87,607 : 0 : 155,848,175:62%\n",
      "2025-04-16 11:29:38,514 - INFO: 2021-10-12 20:03:11 : 1,400,000 : 187,607 : 0 : 166,596,325:66%\n",
      "2025-04-16 11:29:41,826 - INFO: 2022-03-30 07:33:05 : 1,500,000 : 287,607 : 0 : 176,951,250:70%\n",
      "2025-04-16 11:29:45,109 - INFO: 2022-11-07 17:14:49 : 1,600,000 : 387,607 : 0 : 189,010,150:75%\n",
      "2025-04-16 11:29:48,149 - INFO: 2023-03-29 01:22:35 : 1,700,000 : 487,607 : 0 : 199,234,000:79%\n",
      "2025-04-16 11:29:51,011 - INFO: 2023-08-11 13:15:58 : 1,800,000 : 587,607 : 0 : 209,195,700:83%\n",
      "2025-04-16 11:29:53,845 - INFO: 2024-01-07 13:25:54 : 1,900,000 : 687,607 : 0 : 219,550,625:87%\n",
      "2025-04-16 11:29:57,116 - INFO: 2024-06-19 11:47:56 : 2,000,000 : 787,607 : 0 : 240,260,475:95%\n",
      "2025-04-16 11:29:59,900 - INFO: 2024-11-14 06:40:26 : 2,100,000 : 887,607 : 0 : 250,746,475:99%\n",
      "2025-04-16 11:30:00,696 - INFO: Complete : 2,128,327 : 915,935 : 0\n",
      "2025-04-16 11:30:00,696 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\MDMA_submissions.zst : Output: 1_collect_pushshift\\MDMA_submissions.csv : Is submission True\n",
      "2025-04-16 11:30:02,649 - INFO: 2020-03-01 18:59:57 : 100,000 : 0 : 0 : 39,060,350:57%\n",
      "2025-04-16 11:30:09,631 - INFO: 2023-06-25 04:44:01 : 200,000 : 69,250 : 0 : 62,260,625:91%\n",
      "2025-04-16 11:30:11,178 - INFO: Complete : 234,144 : 103,395 : 0\n",
      "2025-04-16 11:30:11,178 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\meth_comments.zst : Output: 1_collect_pushshift\\meth_comments.csv : Is submission False\n",
      "2025-04-16 11:30:12,345 - INFO: 2020-04-22 23:45:55 : 100,000 : 0 : 0 : 13,893,950:7%\n",
      "2025-04-16 11:30:13,561 - INFO: 2021-01-02 11:49:45 : 200,000 : 1,032 : 0 : 25,952,850:12%\n",
      "2025-04-16 11:30:16,514 - INFO: 2021-07-17 16:55:48 : 300,000 : 101,032 : 0 : 38,011,750:18%\n",
      "2025-04-16 11:30:19,546 - INFO: 2021-12-18 09:36:12 : 400,000 : 201,032 : 0 : 49,284,200:23%\n",
      "2025-04-16 11:30:22,595 - INFO: 2022-04-16 22:03:06 : 500,000 : 301,032 : 0 : 60,949,875:29%\n",
      "2025-04-16 11:30:25,876 - INFO: 2022-08-22 15:03:54 : 600,000 : 401,032 : 0 : 72,222,325:34%\n",
      "2025-04-16 11:30:28,692 - INFO: 2022-12-01 10:51:57 : 700,000 : 501,032 : 0 : 83,494,775:39%\n",
      "2025-04-16 11:30:31,492 - INFO: 2023-02-11 18:08:46 : 800,000 : 601,032 : 0 : 94,374,000:45%\n",
      "2025-04-16 11:30:34,242 - INFO: 2023-04-22 02:40:13 : 900,000 : 701,032 : 0 : 104,991,075:50%\n",
      "2025-04-16 11:30:37,191 - INFO: 2023-07-12 11:43:35 : 1,000,000 : 801,032 : 0 : 125,176,625:59%\n",
      "2025-04-16 11:30:39,858 - INFO: 2023-10-09 22:45:48 : 1,100,000 : 901,032 : 0 : 134,876,175:64%\n",
      "2025-04-16 11:30:42,791 - INFO: 2024-01-03 21:38:28 : 1,200,000 : 1,001,032 : 0 : 154,799,575:73%\n",
      "2025-04-16 11:30:45,490 - INFO: 2024-03-20 01:21:57 : 1,300,000 : 1,101,032 : 0 : 164,630,200:78%\n",
      "2025-04-16 11:30:48,163 - INFO: 2024-05-27 15:38:39 : 1,400,000 : 1,201,032 : 0 : 174,460,825:82%\n",
      "2025-04-16 11:30:51,057 - INFO: 2024-08-15 20:12:48 : 1,500,000 : 1,301,032 : 0 : 194,384,225:92%\n",
      "2025-04-16 11:30:53,789 - INFO: 2024-11-03 09:07:38 : 1,600,000 : 1,401,032 : 0 : 204,345,925:97%\n",
      "2025-04-16 11:30:55,537 - INFO: Complete : 1,662,668 : 1,463,701 : 0\n",
      "2025-04-16 11:30:55,540 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\meth_submissions.zst : Output: 1_collect_pushshift\\meth_submissions.csv : Is submission True\n",
      "2025-04-16 11:31:00,089 - INFO: 2023-04-12 13:36:20 : 100,000 : 74,064 : 0 : 40,108,950:67%\n",
      "2025-04-16 11:31:07,038 - INFO: Complete : 169,593 : 143,658 : 0\n",
      "2025-04-16 11:31:07,038 - INFO: Input: G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\\opiates_comments.zst : Output: 1_collect_pushshift\\opiates_comments.csv : Is submission False\n",
      "2025-04-16 11:31:08,054 - INFO: 2012-09-30 18:45:23 : 100,000 : 0 : 0 : 31,589,075:5%\n",
      "2025-04-16 11:31:08,688 - INFO: 2013-06-09 11:49:23 : 200,000 : 0 : 0 : 31,589,075:5%\n",
      "2025-04-16 11:31:09,719 - INFO: 2013-09-19 04:15:28 : 300,000 : 0 : 0 : 62,391,700:9%\n",
      "2025-04-16 11:31:10,324 - INFO: 2013-12-27 03:49:05 : 400,000 : 0 : 0 : 62,391,700:9%\n",
      "2025-04-16 11:31:11,421 - INFO: 2014-03-21 22:07:11 : 500,000 : 0 : 0 : 93,194,325:13%\n",
      "2025-04-16 11:31:12,090 - INFO: 2014-07-03 23:14:38 : 600,000 : 0 : 0 : 93,194,325:13%\n",
      "2025-04-16 11:31:13,121 - INFO: 2014-10-21 22:04:35 : 700,000 : 0 : 0 : 125,045,550:18%\n",
      "2025-04-16 11:31:13,737 - INFO: 2015-02-10 15:09:42 : 800,000 : 0 : 0 : 125,045,550:18%\n",
      "2025-04-16 11:31:14,793 - INFO: 2015-05-24 18:36:55 : 900,000 : 0 : 0 : 157,027,850:22%\n",
      "2025-04-16 11:31:15,413 - INFO: 2015-08-31 16:46:06 : 1,000,000 : 0 : 0 : 157,027,850:22%\n",
      "2025-04-16 11:31:15,970 - INFO: 2015-11-08 19:16:16 : 1,100,000 : 0 : 0 : 157,027,850:22%\n",
      "2025-04-16 11:31:16,937 - INFO: 2016-01-12 04:50:18 : 1,200,000 : 0 : 0 : 189,403,375:27%\n",
      "2025-04-16 11:31:17,497 - INFO: 2016-03-19 08:06:51 : 1,300,000 : 0 : 0 : 189,403,375:27%\n",
      "2025-04-16 11:31:18,457 - INFO: 2016-06-08 10:04:10 : 1,400,000 : 0 : 0 : 221,516,750:32%\n",
      "2025-04-16 11:31:19,020 - INFO: 2016-08-23 05:21:54 : 1,500,000 : 0 : 0 : 221,516,750:32%\n",
      "2025-04-16 11:31:19,553 - INFO: 2016-10-28 00:26:07 : 1,600,000 : 0 : 0 : 221,516,750:32%\n",
      "2025-04-16 11:31:20,536 - INFO: 2016-12-28 00:15:08 : 1,700,000 : 0 : 0 : 253,105,825:36%\n",
      "2025-04-16 11:31:21,086 - INFO: 2017-02-19 05:46:40 : 1,800,000 : 0 : 0 : 253,105,825:36%\n",
      "2025-04-16 11:31:22,009 - INFO: 2017-04-13 00:27:52 : 1,900,000 : 0 : 0 : 279,451,900:40%\n",
      "2025-04-16 11:31:22,551 - INFO: 2017-06-04 23:15:31 : 2,000,000 : 0 : 0 : 279,451,900:40%\n",
      "2025-04-16 11:31:23,470 - INFO: 2017-07-28 06:41:06 : 2,100,000 : 0 : 0 : 302,521,100:43%\n",
      "2025-04-16 11:31:24,023 - INFO: 2017-09-14 04:27:24 : 2,200,000 : 0 : 0 : 302,521,100:43%\n",
      "2025-04-16 11:31:24,569 - INFO: 2017-11-04 22:51:10 : 2,300,000 : 0 : 0 : 302,521,100:43%\n",
      "2025-04-16 11:31:25,510 - INFO: 2017-12-28 08:55:10 : 2,400,000 : 0 : 0 : 325,328,150:46%\n",
      "2025-04-16 11:31:26,072 - INFO: 2018-02-19 02:41:47 : 2,500,000 : 0 : 0 : 325,328,150:46%\n",
      "2025-04-16 11:31:27,088 - INFO: 2018-04-11 02:28:08 : 2,600,000 : 0 : 0 : 344,334,025:49%\n",
      "2025-04-16 11:31:28,106 - INFO: 2018-06-01 17:07:36 : 2,700,000 : 0 : 0 : 360,718,400:51%\n",
      "2025-04-16 11:31:28,855 - INFO: 2018-07-18 21:43:32 : 2,800,000 : 0 : 0 : 360,718,400:51%\n",
      "2025-04-16 11:31:29,968 - INFO: 2018-09-06 04:48:04 : 2,900,000 : 0 : 0 : 376,054,175:54%\n",
      "2025-04-16 11:31:31,118 - INFO: 2018-10-27 22:25:30 : 3,000,000 : 0 : 0 : 391,127,800:56%\n",
      "2025-04-16 11:31:32,252 - INFO: 2018-12-18 01:18:58 : 3,100,000 : 0 : 0 : 405,939,275:58%\n",
      "2025-04-16 11:31:33,400 - INFO: 2019-01-31 18:20:40 : 3,200,000 : 0 : 0 : 420,750,750:60%\n",
      "2025-04-16 11:31:34,522 - INFO: 2019-03-21 05:49:47 : 3,300,000 : 0 : 0 : 434,906,850:62%\n",
      "2025-04-16 11:31:35,635 - INFO: 2019-05-15 23:09:46 : 3,400,000 : 0 : 0 : 449,587,250:64%\n",
      "2025-04-16 11:31:36,468 - INFO: 2019-08-07 21:12:49 : 3,500,000 : 0 : 0 : 449,587,250:64%\n",
      "2025-04-16 11:31:37,651 - INFO: 2019-11-15 22:41:36 : 3,600,000 : 0 : 0 : 463,874,425:66%\n",
      "2025-04-16 11:31:38,822 - INFO: 2020-02-08 14:56:01 : 3,700,000 : 0 : 0 : 477,899,450:68%\n",
      "2025-04-16 11:31:39,984 - INFO: 2020-05-04 10:28:32 : 3,800,000 : 0 : 0 : 490,744,800:70%\n",
      "2025-04-16 11:31:41,167 - INFO: 2020-08-27 12:44:38 : 3,900,000 : 0 : 0 : 504,245,525:72%\n",
      "2025-04-16 11:31:42,334 - INFO: 2020-12-27 17:14:31 : 4,000,000 : 0 : 0 : 517,877,325:74%\n",
      "2025-04-16 11:31:45,636 - INFO: 2021-04-26 16:21:05 : 4,100,000 : 96,580 : 0 : 545,796,300:78%\n",
      "2025-04-16 11:31:48,283 - INFO: Decoding error with 134,217,728 bytes, reading another chunk\n",
      "2025-04-16 11:31:49,865 - INFO: 2021-08-29 01:32:57 : 4,200,000 : 196,580 : 0 : 571,880,225:82%\n",
      "2025-04-16 11:31:52,482 - INFO: 2021-12-04 18:27:58 : 4,300,000 : 296,580 : 0 : 571,880,225:82%\n",
      "2025-04-16 11:31:55,834 - INFO: 2022-03-27 04:47:30 : 4,400,000 : 396,580 : 0 : 584,856,650:83%\n",
      "2025-04-16 11:31:59,015 - INFO: 2022-07-30 22:55:26 : 4,500,000 : 496,580 : 0 : 597,702,000:85%\n",
      "2025-04-16 11:32:02,214 - INFO: 2022-11-09 22:14:49 : 4,600,000 : 596,580 : 0 : 611,071,650:87%\n",
      "2025-04-16 11:32:05,366 - INFO: 2023-02-11 16:19:44 : 4,700,000 : 696,580 : 0 : 624,703,450:89%\n",
      "2025-04-16 11:32:08,528 - INFO: 2023-05-30 21:28:42 : 4,800,000 : 796,580 : 0 : 636,500,200:91%\n",
      "2025-04-16 11:32:11,957 - INFO: 2023-09-27 23:20:32 : 4,900,000 : 896,580 : 0 : 659,045,100:94%\n",
      "2025-04-16 11:32:14,959 - INFO: 2024-02-17 07:50:04 : 5,000,000 : 996,580 : 0 : 670,317,550:96%\n",
      "2025-04-16 11:32:18,311 - INFO: 2024-08-17 19:44:14 : 5,100,000 : 1,096,580 : 0 : 692,993,525:99%\n",
      "2025-04-16 11:32:20,978 - INFO: Complete : 5,184,595 : 1,181,176 : 0\n"
     ]
    }
   ],
   "source": [
    "import zstandard\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import logging.handlers\n",
    "\n",
    "\n",
    "# put the path to the input file, or a folder of files to process all\n",
    "input_file = r\"G:\\My Drive\\Fentanyl Research\\Reddit\\subreddits24\"\n",
    "\n",
    "# put the name or path to the output file. The file extension from below will be added automatically. If the input file is a folder, the output will be treated as a folder as well\n",
    "output_file = \"raw_reddit_data\"\n",
    "\n",
    "# the format to output in, pick from the following options\n",
    "#   zst: same as the input, a zstandard compressed ndjson file. Can be read by the other scripts in the repo\n",
    "#   txt: an ndjson file, which is a text file with a separate json object on each line. Can be opened by any text editor\n",
    "#   csv: a comma separated value file. Can be opened by a text editor or excel\n",
    "# WARNING READ THIS: if you use txt or csv output on a large input file without filtering out most of the rows, the resulting file will be extremely large. Usually about 7 times as large as the compressed input file\n",
    "output_format = \"csv\"\n",
    "# override the above format and output only this field into a text file, one per line. Useful if you want to make a list of authors or ids. See the examples below\n",
    "# any field that's in the dump is supported, but useful ones are\n",
    "#   author: the username of the author\n",
    "#   id: the id of the submission or comment\n",
    "#   link_id: only for comments, the fullname of the submission the comment is associated with\n",
    "#   parent_id: only for comments, the fullname of the parent of the comment. Either another comment or the submission if it's top level\n",
    "single_field = None\n",
    "# the fields in the file are different depending on whether it has comments or submissions. If we're writing a csv, we need to know which fields to write.\n",
    "# set this to true to write out to the log every time there's a bad line, set to false if you're expecting only some of the lines to match the key\n",
    "write_bad_lines = True\n",
    "\n",
    "# only output items between these two dates\n",
    "from_date = datetime.strptime(\"2020-12-31\", \"%Y-%m-%d\")\n",
    "to_date = datetime.strptime(\"2025-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# change this to field = None if you don't want to filter by anything\n",
    "field = None\n",
    "values = ['']\n",
    "# if you have a long list of values, you can put them in a file and put the filename here. If set this overrides the value list above\n",
    "# if this list is very large, it could greatly slow down the process\n",
    "values_file = None\n",
    "exact_match = False\n",
    "\n",
    "\n",
    "# sets up logging to the console as well as a file\n",
    "log = logging.getLogger(\"bot\")\n",
    "log.setLevel(logging.INFO)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s')\n",
    "log_str_handler = logging.StreamHandler()\n",
    "log_str_handler.setFormatter(log_formatter)\n",
    "log.addHandler(log_str_handler)\n",
    "if not os.path.exists(\"logs\"):\n",
    "\tos.makedirs(\"logs\")\n",
    "log_file_handler = logging.handlers.RotatingFileHandler(os.path.join(\"logs\", \"bot.log\"), maxBytes=1024*1024*16, backupCount=5)\n",
    "log_file_handler.setFormatter(log_formatter)\n",
    "log.addHandler(log_file_handler)\n",
    "\n",
    "\n",
    "def write_line_zst(handle, line):\n",
    "\thandle.write(line.encode('utf-8'))\n",
    "\thandle.write(\"\\n\".encode('utf-8'))\n",
    "\n",
    "\n",
    "def write_line_json(handle, obj):\n",
    "\thandle.write(json.dumps(obj))\n",
    "\thandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "def write_line_single(handle, obj, field):\n",
    "\tif field in obj:\n",
    "\t\thandle.write(obj[field])\n",
    "\telse:\n",
    "\t\tlog.info(f\"{field} not in object {obj['id']}\")\n",
    "\thandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def write_line_csv(writer, obj, is_submission):\n",
    "    output_list = []\n",
    "    output_list.append(str(obj['score']))\n",
    "    output_list.append(datetime.fromtimestamp(int(obj['created_utc'])).strftime(\"%Y-%m-%d\"))\n",
    "    if is_submission:\n",
    "        output_list.append(obj['title'])\n",
    "        output_list.append(obj['id'])  # Add id for submissions\n",
    "    else:\n",
    "        output_list.append(obj['link_id'])  # Add link_id for comments\n",
    "        output_list.append(obj['id'])  # Add id for comments\n",
    "        output_list.append(str(obj['is_submitter']))  # Add is_submitter for comments\n",
    "    output_list.append(f\"u/{obj['author']}\")\n",
    "    output_list.append(f\"https://www.reddit.com{obj['permalink']}\")\n",
    "    if is_submission:\n",
    "        if obj['is_self']:\n",
    "            if 'selftext' in obj:\n",
    "                output_list.append(obj['selftext'])\n",
    "            else:\n",
    "                output_list.append(\"\")\n",
    "        else:\n",
    "            output_list.append(obj['url'])\n",
    "    else:\n",
    "        output_list.append(obj['body'])\n",
    "    writer.writerow(output_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_and_decode(reader, chunk_size, max_window_size, previous_chunk=None, bytes_read=0):\n",
    "\tchunk = reader.read(chunk_size)\n",
    "\tbytes_read += chunk_size\n",
    "\tif previous_chunk is not None:\n",
    "\t\tchunk = previous_chunk + chunk\n",
    "\ttry:\n",
    "\t\treturn chunk.decode()\n",
    "\texcept UnicodeDecodeError:\n",
    "\t\tif bytes_read > max_window_size:\n",
    "\t\t\traise UnicodeError(f\"Unable to decode frame after reading {bytes_read:,} bytes\")\n",
    "\t\tlog.info(f\"Decoding error with {bytes_read:,} bytes, reading another chunk\")\n",
    "\t\treturn read_and_decode(reader, chunk_size, max_window_size, chunk, bytes_read)\n",
    "\n",
    "\n",
    "def read_lines_zst(file_name):\n",
    "\twith open(file_name, 'rb') as file_handle:\n",
    "\t\tbuffer = ''\n",
    "\t\treader = zstandard.ZstdDecompressor(max_window_size=2**31).stream_reader(file_handle)\n",
    "\t\twhile True:\n",
    "\t\t\tchunk = read_and_decode(reader, 2**27, (2**29) * 2)\n",
    "\n",
    "\t\t\tif not chunk:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tlines = (buffer + chunk).split(\"\\n\")\n",
    "\n",
    "\t\t\tfor line in lines[:-1]:\n",
    "\t\t\t\tyield line.strip(), file_handle.tell()\n",
    "\n",
    "\t\t\tbuffer = lines[-1]\n",
    "\n",
    "\t\treader.close()\n",
    "\n",
    "\n",
    "def process_file(input_file, output_file, output_format, field, values, from_date, to_date, single_field, exact_match):\n",
    "\toutput_path = f\"{output_file}.{output_format}\"\n",
    "\tis_submission = \"submission\" in input_file\n",
    "\tlog.info(f\"Input: {input_file} : Output: {output_path} : Is submission {is_submission}\")\n",
    "\twriter = None\n",
    "\tif output_format == \"zst\":\n",
    "\t\thandle = zstandard.ZstdCompressor().stream_writer(open(output_path, 'wb'))\n",
    "\telif output_format == \"txt\":\n",
    "\t\thandle = open(output_path, 'w', encoding='UTF-8')\n",
    "\telif output_format == \"csv\":\n",
    "\t\thandle = open(output_path, 'w', encoding='UTF-8', newline='')\n",
    "\t\twriter = csv.writer(handle)\n",
    "\telse:\n",
    "\t\tlog.error(f\"Unsupported output format {output_format}\")\n",
    "\t\tsys.exit()\n",
    "\n",
    "\tfile_size = os.stat(input_file).st_size\n",
    "\tcreated = None\n",
    "\tmatched_lines = 0\n",
    "\tbad_lines = 0\n",
    "\ttotal_lines = 0\n",
    "\tfor line, file_bytes_processed in read_lines_zst(input_file):\n",
    "\t\ttotal_lines += 1\n",
    "\t\tif total_lines % 100000 == 0:\n",
    "\t\t\tlog.info(f\"{created.strftime('%Y-%m-%d %H:%M:%S')} : {total_lines:,} : {matched_lines:,} : {bad_lines:,} : {file_bytes_processed:,}:{(file_bytes_processed / file_size) * 100:.0f}%\")\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tobj = json.loads(line)\n",
    "\t\t\tcreated = datetime.utcfromtimestamp(int(obj['created_utc']))\n",
    "\n",
    "\t\t\tif created < from_date:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif created > to_date:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif field is not None:\n",
    "\t\t\t\tfield_value = obj[field].lower()\n",
    "\t\t\t\tmatched = False\n",
    "\t\t\t\tfor value in values:\n",
    "\t\t\t\t\tif exact_match:\n",
    "\t\t\t\t\t\tif value == field_value:\n",
    "\t\t\t\t\t\t\tmatched = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif value in field_value:\n",
    "\t\t\t\t\t\t\tmatched = True\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif not matched:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tmatched_lines += 1\n",
    "\t\t\tif output_format == \"zst\":\n",
    "\t\t\t\twrite_line_zst(handle, line)\n",
    "\t\t\telif output_format == \"csv\":\n",
    "\t\t\t\twrite_line_csv(writer, obj, is_submission)\n",
    "\t\t\telif output_format == \"txt\":\n",
    "\t\t\t\tif single_field is not None:\n",
    "\t\t\t\t\twrite_line_single(handle, obj, single_field)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twrite_line_json(handle, obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlog.info(f\"Something went wrong, invalid output format {output_format}\")\n",
    "\t\texcept (KeyError, json.JSONDecodeError) as err:\n",
    "\t\t\tbad_lines += 1\n",
    "\t\t\tif write_bad_lines:\n",
    "\t\t\t\tif isinstance(err, KeyError):\n",
    "\t\t\t\t\tlog.warning(f\"Key {field} is not in the object: {err}\")\n",
    "\t\t\t\telif isinstance(err, json.JSONDecodeError):\n",
    "\t\t\t\t\tlog.warning(f\"Line decoding failed: {err}\")\n",
    "\t\t\t\tlog.warning(line)\n",
    "\n",
    "\thandle.close()\n",
    "\tlog.info(f\"Complete : {total_lines:,} : {matched_lines:,} : {bad_lines:,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tif single_field is not None:\n",
    "\t\tlog.info(\"Single field output mode, changing output file format to txt\")\n",
    "\t\toutput_format = \"txt\"\n",
    "\n",
    "\tif values_file is not None:\n",
    "\t\tvalues = []\n",
    "\t\twith open(values_file, 'r') as values_handle:\n",
    "\t\t\tfor value in values_handle:\n",
    "\t\t\t\tvalues.append(value.strip().lower())\n",
    "\t\tlog.info(f\"Loaded {len(values)} from values file {values_file}\")\n",
    "\telse:\n",
    "\t\tvalues = [value.lower() for value in values]  # convert to lowercase\n",
    "\n",
    "\tlog.info(f\"Filtering field: {field}\")\n",
    "\tif len(values) <= 20:\n",
    "\t\tlog.info(f\"On values: {','.join(values)}\")\n",
    "\telse:\n",
    "\t\tlog.info(f\"On values:\")\n",
    "\t\tfor value in values:\n",
    "\t\t\tlog.info(value)\n",
    "\tlog.info(f\"Exact match {('on' if exact_match else 'off')}. Single field {single_field}.\")\n",
    "\tlog.info(f\"From date {from_date.strftime('%Y-%m-%d')} to date {to_date.strftime('%Y-%m-%d')}\")\n",
    "\tlog.info(f\"Output format set to {output_format}\")\n",
    "\n",
    "\tinput_files = []\n",
    "\tif os.path.isdir(input_file):\n",
    "\t\tif not os.path.exists(output_file):\n",
    "\t\t\tos.makedirs(output_file)\n",
    "\t\tfor file in os.listdir(input_file):\n",
    "\t\t\tif not os.path.isdir(file) and file.endswith(\".zst\"):\n",
    "\t\t\t\tinput_name = os.path.splitext(os.path.splitext(os.path.basename(file))[0])[0]\n",
    "\t\t\t\tinput_files.append((os.path.join(input_file, file), os.path.join(output_file, input_name)))\n",
    "\telse:\n",
    "\t\tinput_files.append((input_file, output_file))\n",
    "\tlog.info(f\"Processing {len(input_files)} files\")\n",
    "\tfor file_in, file_out in input_files:\n",
    "\t\tprocess_file(file_in, file_out, output_format, field, values, from_date, to_date, single_field, exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
